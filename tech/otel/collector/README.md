# OpenTelemety Collector

- Vendor-agnostic way to receive, process and export telemetry data.
![otel-colelctor](../../../images/otel-collector.svg)

## Introduction

- The `OpenTelemetry Collector` offers a *vendor-agnostic implementation* of how to `receive`, `process` and `export` telemetry data. It removes the need to *run*, *operate*, and *maintain* **multiple agents/collectors**. This works with improved *scalability* and supports open source observability data formats (e.g. `Jaeger`, `Prometheus`, `Fluent Bit`, etc.) sending to one or more open source or *commercial backends*.

## Objectives

- *Usability*: Reasonable default configuration, supports popular protocols, runs and collects out of the box.
- *Performance*: Highly stable and performant under varying loads and configurations.
- *Observability*: An exemplar of an observable service.
- *Extensibility*: Customizable without touching the core code.
- *Unification*: Single codebase, deployable as an agent or collector with support for `traces`, `metrics`, and `logs`.

## Installation

- Install from source or download pre-built binaries from the [releases page](https://github.com/open-telemetry/opentelemetry-collector-releases) and follow steps in [here](https://www.fosstechnix.com/setup-opentelemetry-collector-on-ubuntu/)

```bash
wget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/{VERSION}/{VERSION_FILE}.deb
sudo dpkg -i {VERSION_FILE}.deb
```

File `/etc/otelcol-contrib/config.yaml` configuration `OpenTelemety Collector` can be found [here](https://opentelemetry.io/docs/collector/configuration/)

- For Docker installation, refer to the [Docker Hub page](https://hub.docker.com/r/otel/opentelemetry-collector). You can install by docker-compose as well:

```yaml
services:
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.138.0
    container_name: otel-collector
    command: ["--config=/etc/otel/config.yaml"]
    volumes:
      - ./collector-config.yaml:/etc/otel/config.yaml
    ports:
      - "4317:4317" # OTLP gRPC
      - "13133:13133" # Health check
      - "1777:1777" # pprof
      - "55679:55679" # zpages
      - "8888:8888" # prometheus scrape metrics from otel collector
      - "9464:9464" # Prometheus scrape metrics from apllication generated by otel
```

## Deployment

- You can deployment `OpenTelemetry Collector` with `No Collector`, `Agent`, `Gateway`, and `Collector` modes. More details can be found [here](https://opentelemetry.io/docs/collector/deployment/)
- Gateway mode is used to receive data from multiple agents and export it to one or more backends. Agent mode is used to collect data from a single application and send it to a gateway or backend. You can use `Nginx` for load balancing as an "out of the box"

```nginx
server {
    listen 4317 http2;
    server_name _;

    location / {
            grpc_pass      grpc://collector4317;
            grpc_next_upstream     error timeout invalid_header http_500;
            grpc_connect_timeout   2;
            grpc_set_header        Host            $host;
            grpc_set_header        X-Real-IP       $remote_addr;
            grpc_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}

server {
    listen 4318;
    server_name _;

    location / {
            proxy_pass      http://collector4318;
            proxy_redirect  off;
            proxy_next_upstream     error timeout invalid_header http_500;
            proxy_connect_timeout   2;
            proxy_set_header        Host            $host;
            proxy_set_header        X-Real-IP       $remote_addr;
            proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}

upstream collector4317 {
    server collector1:4317;
    server collector2:4317;
    server collector3:4317;
}

upstream collector4318 {
    server collector1:4318;
    server collector2:4318;
    server collector3:4318;
}
```

- You can use load-balancing exporter with
  - Static list of endpoints:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

exporters:
  loadbalancing:
    protocol:
      otlp:
        tls:
          insecure: true
    resolver:
      static:
        hostnames:
          - collector-1.example.com:4317
          - collector-2.example.com:5317
          - collector-3.example.com

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [loadbalancing]
```

- DNS:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

exporters:
  loadbalancing:
    protocol:
      otlp:
        tls:
          insecure: true
    resolver:
      dns:
        hostname: collectors.example.com

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [loadbalancing]
```

- DNS with Service:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

exporters:
  loadbalancing:
    routing_key: service
    protocol:
      otlp:
        tls:
          insecure: true
    resolver:
      dns:
        hostname: collectors.example.com
        port: 5317

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [loadbalancing]
```

## Configuration

### Configuration Structure

- The structure of any Collector configuration file consists of four classes of pipeline components that access telemetry data:
  - [Receivers](https://opentelemetry.io/docs/collector/configuration/#receivers): These components receive data from various sources, such as applications or other collectors.
  - [Processors](https://opentelemetry.io/docs/collector/configuration/#processors): These components process the data, which may include filtering, batching, or
  - [Exporters](https://opentelemetry.io/docs/collector/configuration/#exporters): These components send the processed data to various backends or storage systems.
  - [Connectors](https://opentelemetry.io/docs/collector/configuration/#connectors): These components connect different parts of the pipeline, allowing data to flow between receivers, processors, and exporters.

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
processors:
  batch:

exporters:
  otlp:
    endpoint: otelcol:4317

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  pprof:
    endpoint: 0.0.0.0:1777
  zpages:
    endpoint: 0.0.0.0:55679

service:
  extensions: [health_check, pprof, zpages]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
```

## Internal telemetry

- You can inspect the health of any `OpenTelemetry Collector` instance by checking its own internal telemetry. Read on to learn about this telemetry and how to configure it to help you [monitor](https://opentelemetry.io/docs/collector/internal-telemetry/#use-internal-telemetry-to-monitor-the-collector) and troubleshoot the [Collector](https://opentelemetry.io/docs/collector/troubleshooting/).

### Configure internal metrics

- You can configure how internal metrics are generated and exposed by the `Collector`. By default, the `Collector` generates basic metrics about itself and exposes them using the `OpenTelemetry Go Prometheus exporter` for scraping at `http://127.0.0.1:8888/metrics`.
- The `Collector` can push its internal metrics to an `OTLP backend` via the following configuration:

```yaml
service:
  telemetry:
    metrics:
    metrics:
      # level: detailed
      readers:
        - pull:
            exporter:
              prometheus:
                host: "0.0.0.0"
                port: 8888
```

### Configure internal logs

- The following configuration can be used to emit internal logs from the Collector to an `OTLP/HTTP backend`:

```yaml
service:
  telemetry:
    logs:
      processors:
        - batch:
            exporter:
              otlp:
                protocol: http/protobuf
                endpoint: https://backend:4318
```

### Configure internal traces

- The Collector does not expose traces by default, but it can be configured to. The following configuration can be used to emit internal traces from the Collector to an `OTLP backend`:

```yaml
service:
  telemetry:
    traces:
      processors:
        - batch:
            exporter:
              otlp:
                protocol: http/protobuf
                endpoint: https://backend:4318
```

## Transforming telemetry

- The `OpenTelemetry Collector` is a convenient place to transform data before sending it to a vendor or other systems. This is frequently done for *data quality*, *governance*, *cost*, and *security reasons*.

### Basic filtering

- ***Processor***: [filter processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor)
- The filter processor allows users to filter telemetry using [OTTL](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/README.md). Telemetry that matches any condition is dropped.
- For example, to *only allow* span data from services app1, app2, and app3 and drop data from all other services:

```yaml
processors:
  filter/ottl:
    error_mode: ignore
    traces:
      span:
        - |
        resource.attributes["service.name"] != "app1" and
        resource.attributes["service.name"] != "app2" and
        resource.attributes["service.name"] != "app3"
```

- To *only drop* spans from a service called service1 while keeping all other spans:

```yaml
processors:
  filter/ottl:
    error_mode: ignore
    traces:
      span:
        - resource.attributes["service.name"] == "service1"
```

### Adding or Deleting Attributes

- ***Processor***: [attributes processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor) or [resource processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor)
- The attributes processor can be used to *update*, *insert*, *delete*, or *replace existing attributes* on metrics or traces. For example, here’s a configuration that adds an attribute called account_id to all spans:

```yaml
processors:
  attributes/accountid:
    actions:
      - key: account_id
        value: 2245
        action: insert
```
