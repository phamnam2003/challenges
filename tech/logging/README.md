# Logging for DevOps

- `Logging` is a *critical component* of `DevOps` practices, providing *insights* into system `performance`, `security`, and `user behavior`.
- `Logging` helps in *troubleshooting issues*, *monitoring applications*, and *ensuring compliance* with regulatory requirements.

## Introduction to Logging

- Managing logs *effectively* is essential for maintaining the *health* and *performance* of applications and infrastructure. When system needs centralized logging:
  - Distributed systems
  - Microservices architecture
  - Collecting logs for analysis and monitoring
  - Trace problems across multiple services, prescription problems
  - The log loss nature of the tool
  - Compliance and auditing requirements
  ...

## Types of Logs

- `Application Logs`: Generated by applications to record events, errors, and other significant occurrences.
- `System Logs`: Generated by the operating system to track system events, hardware issues, and other low-level activities.
- `Access Logs`: Record requests made to web servers, including details like IP addresses, request, Firewall, Nginx, etc.
- `Security, Audit Logs`: Track security-related events, such as login attempts, access control changes, and potential threats.
- `Event Logs`: Capture significant events within an application or system, often used for monitoring and alerting.
- `Trace Logs`: Provide detailed information about the execution of an application, useful for debugging and performance analysis.

## Designing a Logging Strategy

- Structure Logging: JSON format is widely used for its readability and ease of parsing.
- Context/Correlation IDs: Include unique identifiers to trace requests across multiple services.
- Log Levels: Use appropriate log levels (e.g., DEBUG, INFO, WARN, ERROR, etc)
- Don't log sensitive information: Avoid logging sensitive data like passwords, credit card numbers, etc.
- Easily searchable: Ensure logs are indexed and searchable for quick retrieval.

## Log Components

- `Log Producer`: The application or service that generates log entries.
- `Log Collector / Agent`: A tool or service that gathers logs from various sources.
- `Log Transport / Forwarder`: The mechanism used to send logs from producers to collectors (e.g., HTTP, TCP, UDP).
- `Log Processor`: A component that processes logs, such as filtering, parsing, or enriching log data.
- `Log Storage / Index`: A system for storing logs, which can be a database, file system
- `Log Query / Visualization`: Tools or interfaces for querying logs and statistics log.
- `Alerting & Intergration`: Systems that monitor logs for specific patterns or thresholds and trigger alerts.

## Logging Stacks

- `Grafana Loki`: A *horizontally scalable*, *highly available*, *multi-tenant log aggregation* system inspired by Prometheus. It combines with `Promtail` for log collection and `Grafana` for visualization.
- `ELK Stack (Elasticsearch, Logstash, Kibana)`: A popular open-source stack for *searching*, *analyzing*, and *visualizing* log data in real-time. `Filebeat` is often used as a lightweight log shipper. `Elacticsearch` for storage and search, `Logstash` for processing, and `Kibana` for visualization, it is highly performance with search log.
- `VEK stack (Vector, Elasticsearch, Kibana)`: A modern alternative to the ELK stack, `Vector` is a high-performance log collector and processor that can send logs to `Elasticsearch` for storage and `Kibana` for visualization. `Sources` (e.g., files, syslog), `Transforms` (e.g., parsing, filtering), and `Sinks` (e.g., Elasticsearch, Kafka).
- `EFK Stack (Elasticsearch, Fluentd, Kibana)`: Similar to the ELK stack but uses `Fluentd` as the log collector and processor. `Fluentd` is known for its flexibility and ability to handle various data sources and formats.

> [!Notice]
> Logging stacks can be complex to set up and manage, so it's essential to choose the right tools based on your specific requirements and expertise. Many people use `Docker` to install infrastructure logging stacks, but it's `not recommended` for production environments. You need to install directly on the host system for better performance and reliability.

## Setting up infrastructure

- Set `timezone` to local timezone or what timezone you want on all servers to ensure consistent timestamps in logs.

```bash
sudo timedatectl set-timezone Asia/Ho_Chi_Minh
```

- Install [`Elasticsearch`](https://www.elastic.co/docs/deploy-manage/deploy/self-managed/install-elasticsearch-from-archive-on-linux-macos) for log storage and search, `Kibana` to visualization logs from project.
- Custom configuration for `Elasticsearch` in `/etc/elasticsearch/elasticsearch.yml`:

```yaml
cluster.name: elasticsearch-cluster
network.host: 0.0.0.0
http.port: 9200
xpack.security.enabled: true
xpack.monitoring.collection.enabled: true
```

- Custom configuration `JVM Options` for `Elasticsearch` in `/etc/elasticsearch/jvm.options`:

```properties
-Xms4g
-Xmx4g
```

- Change password for `elastic` user:

```bash
sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -i
```

- Testing `Elasticsearch`:

```bash
curl --cacert /etc/elasticsearch/certs/http_ca.crt -u elastic:KHFDPeU6 https://localhost:9200
```

- Install [`Kibana`](https://www.elastic.co/docs/deploy-manage/deploy/self-managed/install-kibana-from-archive-on-linux-macos) for log visualization, custom configuration in `/etc/kibana/kibana.yml`:

```yaml
server.port: 5601
server.host: "0.0.0.0"
server.name: "kibana-server"

elasticsearch.ssl.certificateAuthorities: ["/etc/elasticsearch/certs/http_ca.crt"] # Certificate Authority path from elasticsearch generated before
elasticsearch.hosts: ["https://localhost:9200"]
```

- Get token for `Kibana` to connect to `Elasticsearch`, and get verify code to login `Kibana` web UI:

```bash
# generate token for kibana
sudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana 

# get verify code to login kibana
sudo /usr/share/kibana/bin/kibana-verification-code
```
