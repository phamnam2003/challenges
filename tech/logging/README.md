# Logging for DevOps

- `Logging` is a *critical component* of `DevOps` practices, providing *insights* into system `performance`, `security`, and `user behavior`.
- `Logging` helps in *troubleshooting issues*, *monitoring applications*, and *ensuring compliance* with regulatory requirements.

## Introduction to Logging

- Managing logs *effectively* is essential for maintaining the *health* and *performance* of applications and infrastructure. When system needs centralized logging:
  - Distributed systems
  - Microservices architecture
  - Collecting logs for analysis and monitoring
  - Trace problems across multiple services, prescription problems
  - The log loss nature of the tool
  - Compliance and auditing requirements
  ...

## Types of Logs

- `Application Logs`: Generated by applications to record events, errors, and other significant occurrences.
- `System Logs`: Generated by the operating system to track system events, hardware issues, and other low-level activities.
- `Access Logs`: Record requests made to web servers, including details like IP addresses, request, Firewall, Nginx, etc.
- `Security, Audit Logs`: Track security-related events, such as login attempts, access control changes, and potential threats.
- `Event Logs`: Capture significant events within an application or system, often used for monitoring and alerting.
- `Trace Logs`: Provide detailed information about the execution of an application, useful for debugging and performance analysis.

## Designing a Logging Strategy

- Structure Logging: JSON format is widely used for its readability and ease of parsing.
- Context/Correlation IDs: Include unique identifiers to trace requests across multiple services.
- Log Levels: Use appropriate log levels (e.g., DEBUG, INFO, WARN, ERROR, etc)
- Don't log sensitive information: Avoid logging sensitive data like passwords, credit card numbers, etc.
- Easily searchable: Ensure logs are indexed and searchable for quick retrieval.

## Log Components

- `Log Producer`: The application or service that generates log entries.
- `Log Collector / Agent`: A tool or service that gathers logs from various sources.
- `Log Transport / Forwarder`: The mechanism used to send logs from producers to collectors (e.g., HTTP, TCP, UDP).
- `Log Processor`: A component that processes logs, such as filtering, parsing, or enriching log data.
- `Log Storage / Index`: A system for storing logs, which can be a database, file system
- `Log Query / Visualization`: Tools or interfaces for querying logs and statistics log.
- `Alerting & Intergration`: Systems that monitor logs for specific patterns or thresholds and trigger alerts.

## Logging Stacks

- `Grafana Loki`: A *horizontally scalable*, *highly available*, *multi-tenant log aggregation* system inspired by Prometheus. It combines with `Promtail` for log collection and `Grafana` for visualization.
- `ELK Stack (Elasticsearch, Logstash, Kibana)`: A popular open-source stack for *searching*, *analyzing*, and *visualizing* log data in real-time. `Filebeat` is often used as a lightweight log shipper. `Elacticsearch` for storage and search, `Logstash` for processing, and `Kibana` for visualization, it is highly performance with search log.
- `VEK stack (Vector, Elasticsearch, Kibana)`: A modern alternative to the ELK stack, `Vector` is a high-performance log collector and processor that can send logs to `Elasticsearch` for storage and `Kibana` for visualization. `Sources` (e.g., files, syslog), `Transforms` (e.g., parsing, filtering), and `Sinks` (e.g., Elasticsearch, Kafka).
- `EFK Stack (Elasticsearch, Fluentd, Kibana)`: Similar to the ELK stack but uses `Fluentd` as the log collector and processor. `Fluentd` is known for its flexibility and ability to handle various data sources and formats.
